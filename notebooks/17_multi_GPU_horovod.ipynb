{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47eb464-0dd6-4709-897b-d13fb325b763",
   "metadata": {},
   "source": [
    "# Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c61af-9112-4236-9784-600a9752ca7a",
   "metadata": {},
   "source": [
    "[Horovod](https://github.com/horovod/horovod) is an open source tool originally developed by Uber to support their need for faster deep learning model training. It is a distributed deep learning training framework that works with TensorFlow, Keras, PyTorch, and Apache MXNet. Uber decided to develop a solution that uses [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) for distributed process communication, and the [NVIDIA Collective Communications Library (NCCL)](https://developer.nvidia.com/nccl) for its highly optimized implementation of reductions across distributed processes and nodes.\n",
    "\n",
    "In this session we will take a well known deep learning image classification model, distribute the training data (Fashion-MNIST) across multiplte GPUs on multiple nodes and train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217c7cc-2627-4b8b-b7b0-986e5252c80d",
   "metadata": {},
   "source": [
    "### Horovod's MPI Roots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb25a8fa-87b6-4cbc-b570-aefbd0ee3b65",
   "metadata": {},
   "source": [
    "Horovod assigns a unique numerical ID or **rank** (an MPI concept) to each process executing the program. This rank can be accessed programmatically. As you will see below when writing Horovod code, by identifying a process's rank programmatically in the code we can take steps such as:\n",
    "\n",
    "- Pin that process to its own exclusive GPU.\n",
    "- Utilize a single rank for broadcasting values that need to be used uniformly by all ranks.\n",
    "- Utilize a single rank for collecting and/or reducing values produced by all ranks.\n",
    "- Utilize a single rank for logging or writing to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae64ba24-6b04-496d-a6f7-f5a56cd3fa25",
   "metadata": {},
   "source": [
    "Before we go into modifications required to turn our serial training implementation into a parallel implementation, let's take a look at the fashion_mnist.py file to familiarize ourselves with the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6357be-ff59-475a-bf5d-633afc37bd1d",
   "metadata": {},
   "source": [
    "### The Fashion-MNIST Dataset\n",
    "\n",
    "The [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) is a response to the traditional MNIST dataset, which is often referred to as the \"hello world\" of machine learning. The original MNIST dataset consists of 60,000 pictures of handwritten digits, 0-9. One of the downsides of this dataset is its simplicity. Good performance of a model on the dataset does not indicate that the model will perform well on a more complicated set of images.\n",
    "\n",
    "The Fashion-MNIST dataset was created to be a moderately more complex image classification challenge. It follows the same format as the original MNIST set, with 10 categories and 60,000 training images, each 28x28 pixels (plus 10,000 testing images). We'll be training on this dataset for this lab. \n",
    "\n",
    "<img src=\"./images/Fashion MNIST.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c738b7d-dd5f-41b2-999e-4b6b04a32902",
   "metadata": {},
   "source": [
    "## Modify the Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced85db-3cdc-45c5-afc0-decdb06c0372",
   "metadata": {},
   "source": [
    "You are going to start making modifications to the training script. In case you want to go back to the original Python file, I've provided a copy for you named fashion_mnist_original.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c4372-4961-4110-a297-b24703f76ddd",
   "metadata": {},
   "source": [
    "### 1. Initialize Horovod and Select the GPU to Run On"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea785630-f6d5-46df-9f08-4abff99f9d4d",
   "metadata": {},
   "source": [
    "Let's begin by importing Horovod by it's common alias `hvd`. We will be using Tensorflow with Keras. Horovod has a backend for each implementation it supports, including Tensorflow and Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aaece1-465e-4347-80c0-ac931076d9c7",
   "metadata": {},
   "source": [
    "**Exercise**: Add `import horovod.tensorflow.keras as hvd` to the training script and initialize Horovod before the argument parsing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b3b48-2e76-4326-a84e-407281c4d7eb",
   "metadata": {},
   "source": [
    "```python\n",
    "# Horovod: initialize Horovod.\n",
    "hvd.init()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f3c2d-1a1a-4913-b09c-4ac82f696dc1",
   "metadata": {},
   "source": [
    "(look for the `TODO: Step 1` lines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0e40db-5401-4094-9fe8-2a8d43b474f0",
   "metadata": {},
   "source": [
    "With Horovod, which can run multiple processes across multiple GPUs, you typically use a single GPU per training process. Horovod uses much of the MPI nomenclature. The concept of a **rank** in MPI is of a unique process ID.\n",
    "Schematically, let's look at how MPI can run multiple GPU processes across multiple nodes. Note how each process, or rank, is pinned to a specific GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e62be-7dac-4559-b0c0-f9ef474f0ad3",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/16640218/53518255-7d5fc300-3a85-11e9-8bf3-5d0e8913c14f.png\" width=\"400\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371d3d28-e956-4554-b68c-d8a3a5eb287a",
   "metadata": {},
   "source": [
    "With this method we do not have to deal with placing specific data on specific GPUs. Instead, you just specify which GPU you would like to use in the beginning of your script. \n",
    "\n",
    "On the NVIDIA platform, CUDA, if we have N GPUs they are uniquely numbered from 0 to N-1.\n",
    "\n",
    "```python\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c25fdb8-64b1-441b-9209-da2a511d8abe",
   "metadata": {},
   "source": [
    "`list_physical_devices` returns an array of the GPUs (GPUS are \"devices\", in TensorFlow) that this TensorFlow process can see and from which we must select one to use.\n",
    "\n",
    "In this example we are using the `set_memory_growth` option. This tells TensorFlow to start with the minimum amount of GPU memory needed to start, and to allocate more on demand (like when the network is initialized). This is not strictly related to Horovod, but is a commonly used option when working with GPUs in TensorFlow. Since we need to request a fixed amount of resources on the VSC, this does not have an effect (it doesn't hurt though and is general good practice)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f9cbe-3485-4151-8dc3-4babdd184b7b",
   "metadata": {},
   "source": [
    "Now let's modify the above code such that Horovod can automatically do the right thing for any number of training processes. Programmatically, we can arbitrarily select the GPU that corresponds to the Horovod rank and use that one. Since we might be using multiple nodes, and the Horovod rank is a unique identifier across all ranks in the training process, we want to identify our rank locally on the node, which is provided by the `local_rank` specifier. Then we provide the `local_rank` to the function `set_visible_devices` which controls the set of GPUs that are available to that rank:\n",
    "\n",
    "```python\n",
    "# Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "```\n",
    "\n",
    "You can have a look at the documentation for both functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db00a1-363f-4536-a146-2371a94b5e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import horovod.tensorflow.keras as hvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd57040-058d-4aa3-b607-50f448851145",
   "metadata": {},
   "outputs": [],
   "source": [
    "?hvd.rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7362d6-bf2a-4970-a2d9-172900f63a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "?hvd.local_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77df00f-0815-49c6-97da-77dbd9b69937",
   "metadata": {},
   "source": [
    "**Exercise**: with this knowledge in hand, edit `fashion_mnist.py` to pin one GPU to each rank using its local rank ID, immediately after where you have already initialized Horovod.\n",
    "\n",
    "Look for `TODO: Step 1` lines in the code. If you get stuck, refer to `solutions/fashion_mnist_after_step_01.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7904c1-0609-4943-92ac-176c4e5561b2",
   "metadata": {},
   "source": [
    "### 2. Print Verbose Logs Only on the First Worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45f97e-8eb7-49d5-a812-f99906370e06",
   "metadata": {},
   "source": [
    "We do not want that all N TensorFlow processes print their progress the output. We only want to see the state of the output once at any given time. To accomplish this, we can arbitrarily select a single rank to display the training progress. By convention, we typically call rank 0 the \"root\" rank and use it for logistical work such as I/O when only one rank is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8754dcc9-2a8d-4821-8af1-cfb03689d9fd",
   "metadata": {},
   "source": [
    "**Exercise**: Edit `fashion_mnist.py` so that you only set `verbose = 1` if it is the first worker (with rank equal to 0) executing the code.\n",
    "\n",
    "Look for `TODO: Step 2` in `fashion_mnist.py`. If you get stuck, refer to `solutions/fashion_mnist_after_step_02.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2096b3-1388-4959-950a-60713d73a5a9",
   "metadata": {},
   "source": [
    "### 3. Add Distributed Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458c7f9-8bd9-43c8-b6f2-3a880f211394",
   "metadata": {},
   "source": [
    "If we left it at that, each process would be running completely independently, which is not data parallel training, but multiple processes running serial training at the same time. The key step to make the training data parallel is to average out gradients across all workers, so that all workers are updating with the same gradients and thus moving in the same direction. Horovod implements an operation that averages gradients across workers. Deploying this in your code is very straightforward and just requires wrapping an existing optimizer (`tensorflow.keras.optimizers.Optimizer`) with a Horovod distributed optimizer (`horovod.tensorflow.keras.DistributedOptimizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6015c989-cf5a-479c-9a27-5bd408483c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "?hvd.DistributedOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47789599-1c1c-4e04-ab65-e380daf3cad5",
   "metadata": {},
   "source": [
    "**Exercise**: wrap the optimizer (`opt` in `fashion_mnist.py`) with a Horovod distributed optimizer.\n",
    "\n",
    "Look for `TODO: Step 3` in `fashion_mnist.py`. If you get stuck, refer to `solutions/fashion_mnist_after_step_03.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0445382c-90b1-449d-8c6e-7186430ac25a",
   "metadata": {},
   "source": [
    "### 4. Initialize Random Weights on Only One Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec89fe-ee46-4ec1-9a90-571a075531b5",
   "metadata": {},
   "source": [
    "Data parallel stochastic gradient descent, at least in its traditionally defined sequential algorithm, requires weights to be synchronized between all processors. We already know that this is accomplished for backpropagation by averaging out the gradients among all processors prior to the weight updates. Then the only other required step is for the weights to be synchronized initially. Assuming we start from the beginning of the training, this means that every processor needs to have the same random weights.\n",
    "\n",
    "The first worker needs to broadcast parameters to the rest of the workers.  We will use `horovod.tensorflow.keras.callbacks.BroadcastGlobalVariablesCallback` to make this happen. Execute the following cell to get more information about the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108f67b-0d88-4e97-add9-7db315cac44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "?hvd.callbacks.BroadcastGlobalVariablesCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3cd4e-4ec1-47ac-a5ee-a8c8d81b7a7e",
   "metadata": {},
   "source": [
    "**Exercise**: append this callback to our list of callbacks. Note the argument required for this callback, the rank of the root worker.Introducing this callback causes a TensorFlow warning, which you can disregard.\n",
    "\n",
    "Look for `TODO: Step 4` in `fashion_mnist.py`. If you get stuck, refer to `solutions/fashion_mnist_after_step_04.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b47f0e-ab18-447c-b33f-877c06d2340b",
   "metadata": {},
   "source": [
    "### 5. Modify Training Loop to Execute Fewer Steps Per Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a17bf-1975-4b19-841c-1cd4371e8264",
   "metadata": {},
   "source": [
    "As it stands, we are running the same number of steps per epoch for the serial training implementation. But since we have increased the number of workers by a factor of N, that means we're doing N times more work (when we sum the amount of work done over all processes). Our target was to get the *same* answer in less time (that is, to speed up the training), so we want to keep the total amount of work done the same (that is, to process the same number of examples in the dataset). This means we need to do a factor of N *fewer* steps per epoch, so the number of steps goes to `steps_per_epoch / number_of_workers`.\n",
    "\n",
    "We will also speed up validation by validating `3 * num_test_iterations / number_of_workers` steps on each worker. While we could just do `num_test_iterations / number_of_workers` on each worker to get a linear speedup in the validation, the multiplier **3** provides over-sampling of the validation data and helps to increase the probability that every validation example will be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a52e1f3-9dcd-4e00-8a46-18eb95bd028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "?hvd.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7aa9c1-ec88-4aa8-ac1a-7b4e067d9aa7",
   "metadata": {},
   "source": [
    "**Exercise**: modify the `steps_per_epoch` and `validation_steps` arguments for `model.fit_generator` to follow the plan just outlined. This environment uses Python 3, and each of these arguments expect integers, so take care to round any potential floating point values down to the nearest integer.\n",
    "\n",
    "Look for `TODO: Step 5` in `fashion_mnist.py`. If you get stuck, refer to `solutions/fashion_mnist_after_step_05.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1196a502-a75e-4901-9cbe-0ca1be4f4a0c",
   "metadata": {},
   "source": [
    "### 6. Average Validation Results Among Workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ce980-1bfd-40a5-90ad-3112fcabd66e",
   "metadata": {},
   "source": [
    "Since we are not validating the full dataset on each worker anymore, each worker will have different validation results. To improve validation metric quality and reduce variance, we will average validation results among all workers.\n",
    "\n",
    "To do so, we can use `horovod.keras.callbacks.MetricAverageCallback`. Execute the following cell to get more information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb33b1-8782-4f3e-87e9-82004ebc733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "?hvd.callbacks.MetricAverageCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818d2eb5-d585-4e16-83ef-ad3aa7f00b4b",
   "metadata": {},
   "source": [
    "**Exercise**: average the metrics among workers at the end of every epoch by injecting `MetricAverageCallback` after `BroadcastGlobalVariablesCallback`. Please note that this callback must be in the list before other metrics-based callbacks, `ReduceLROnPlateau`, `TensorBoard`, etc.\n",
    "\n",
    "Look for `TODO: Step 6` in `fashion_mnist.py`. If you get stuck, refer to `solutions/fashion_mnist_after_step_06.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ab197-1e36-4787-9dfc-dc2931580056",
   "metadata": {},
   "source": [
    "### 7. Do Checkpointing Logic Only Using the Root Worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba89560-a3f9-46c5-8b6a-bc6c932cfef6",
   "metadata": {},
   "source": [
    "Checkpointing is a common activity in production DL training (for simplicity we are not relying on it in this lab, however). This is extremely important for \"defensive I/O\" purposes (so that if a process or node fails, we don't lose all our work), and is also useful for being able to chart the progress of our training run. Let's think about how that works in the multi-process context.\n",
    "\n",
    "The most important issue is that there can be a race condition while writing the checkpoint to a file. If every rank finishes the epoch at the same time, they might be writing to the same filename, and this could result in corrupted data. But more to the point, we don't even need to do this: by construction in synchronous data parallel SGD, every rank has the same copy of the weights at all times, so only one worker needs to write the checkpoint. As usual, our convention will be that the root worker (rank 0) handles this.\n",
    "\n",
    "For the same reason, if we are restarting from a checkpoint later on, we don't need every rank to read in the checkpoint -- only one rank needs to do so, and then it can broadcast the data to all the other workers. (In that case, all the workers do still need to instantiate the same model as the one in the checkpoint.) We also often don't want every rank to read in the checkpoint -- at large enough scale, having thousands of processes all read from the same file on disk can be inefficient. You might also be in a situation where only one server node has the data available, so the broadcast is necessary in that case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e4f597-e7cd-4830-be6c-588699fd6d71",
   "metadata": {},
   "source": [
    "We already encountered broadcasting in the form of a callback in Step 4. But with Horovod we can take direct control and broadcast (that is, send some data from one processor to every other processor) a specific scalar or tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c9c50-ffdf-4277-bc17-d2620e4a584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "?hvd.broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8d6da-9b1a-4240-8cbd-104a6dc64f0b",
   "metadata": {},
   "source": [
    "The training script has a function `restart_epoch()` which looks for the latest checkpoint we have created and returns the epoch number corresponding to that checkpoint. By default, every rank is resuming from epoch 0 (look for the `resume_from_epoch` variable), which means start from the beginning without loading a checkpoint. We can run the `restart_epoch()` function to actively look for checkpoints, but we only need to do this on one rank, and then we can have that same rank broadcast that epoch number to all other ranks, and also read in the checkpoint and broadcast the checkpointed weights (this is already handled by the `BroadcastGlobalVariablesCallback` we implemented in Step 4).\n",
    "\n",
    "**Exercise**: edit `fashion_mnist.py` so that after\n",
    "\n",
    "```python\n",
    "resume_from_epoch = 0\n",
    "```\n",
    "\n",
    "you:\n",
    "\n",
    "(1) update the `resume_from_epoch` variable on rank 0 (the root process) using the `restart_epoch()` function;\n",
    "\n",
    "(2) broadcast the value of this data to all other processes; and,\n",
    "\n",
    "(3) uncomment the checkpointing callbacks, and make sure they're only appended on rank 0.\n",
    "\n",
    "Use the docstring printed above to assist your work in getting the syntax right.\n",
    "\n",
    "Look for `TODO: Step 7` in `fashion_mnist.py`. If you get stuck, refer to `solutions/fashion_mnist_after_step_07.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5791f-6c76-459f-ae3f-8e13c01bc138",
   "metadata": {},
   "source": [
    "If you make any mistakes, you can simply delete the logs directory and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44074da3-5748-4a09-bc49-30231dfeb048",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f1626-ff94-4de4-a6dc-11c68b18fc73",
   "metadata": {},
   "source": [
    "In this short introduction, we will leave it at that and try to train our model in parallel. For that we will use the the fashion_mnist_solution.py file. We just need to write or complete our job script.\n",
    "However, should you be interested in the rest of the adjustments, feel free to read through the remaining notebook in your own time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
